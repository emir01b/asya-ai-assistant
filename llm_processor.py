import ollama
import requests
from typing import Optional, Dict, List
from config import OLLAMA_BASE_URL, OLLAMA_MODEL, SYSTEM_PROMPT

class LLMProcessor:
    """Ollama kullanarak yerel LLM iÅŸleme sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.base_url = OLLAMA_BASE_URL
        self.model = OLLAMA_MODEL
        self.system_prompt = SYSTEM_PROMPT
        
        print(f"ğŸ¤– LLM sistemi baÅŸlatÄ±lÄ±yor - Model: {self.model}")
        
        # Ollama baÄŸlantÄ±sÄ±nÄ± test et
        if self.check_ollama_connection():
            print("âœ… Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±")
            # Modeli kontrol et ve indir
            self.ensure_model_available()
        else:
            print("âŒ Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±sÄ±z!")
    
    def check_ollama_connection(self) -> bool:
        """Ollama servisinin Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            print(f"âŒ Ollama baÄŸlantÄ± hatasÄ±: {e}")
            return False
    
    def ensure_model_available(self) -> bool:
        """Modelin mevcut olduÄŸundan emin ol, yoksa indir"""
        try:
            # Mevcut modelleri listele
            response = requests.get(f"{self.base_url}/api/tags")
            models = response.json()
            
            model_names = [model['name'] for model in models.get('models', [])]
            
            if self.model not in model_names:
                print(f"ğŸ“¥ Model indiriliyor: {self.model}")
                self.pull_model()
            else:
                print(f"âœ… Model mevcut: {self.model}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Model kontrol hatasÄ±: {e}")
            return False
    
    def pull_model(self) -> bool:
        """Modeli Ollama'ya indir"""
        try:
            print(f"ğŸ“¥ {self.model} modeli indiriliyor... (Bu iÅŸlem zaman alabilir)")
            
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": self.model},
                stream=True,
                timeout=300
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            import json
                            data = json.loads(line)
                            if 'status' in data:
                                print(f"ğŸ“¥ {data['status']}")
                        except:
                            pass
                
                print(f"âœ… Model baÅŸarÄ±yla indirildi: {self.model}")
                return True
            else:
                print(f"âŒ Model indirilemedi: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Model indirme hatasÄ±: {e}")
            return False
    
    def generate_response(self, user_message: str, conversation_context: str = "") -> Optional[str]:
        """KullanÄ±cÄ± mesajÄ±na LLM yanÄ±tÄ± Ã¼ret"""
        try:
            # Tam prompt oluÅŸtur
            full_prompt = self._create_full_prompt(user_message, conversation_context)
            
            print(f"ğŸ¤– LLM'den yanÄ±t bekleniyor...")
            
            # Ollama API ile isteÄŸi gÃ¶nder
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "top_k": 40,
                        "num_predict": 300
                    }
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result.get('response', '').strip()
                
                if ai_response:
                    print(f"âœ… LLM yanÄ±tÄ± alÄ±ndÄ±: {ai_response[:100]}...")
                    return ai_response
                else:
                    print("âš ï¸ LLM boÅŸ yanÄ±t dÃ¶ndÃ¼")
                    return "ÃœzgÃ¼nÃ¼m, ÅŸu anda yanÄ±t veremiyorum. LÃ¼tfen tekrar deneyin."
            else:
                print(f"âŒ LLM API hatasÄ±: {response.status_code}")
                return "Teknik bir sorun yaÅŸÄ±yorum. LÃ¼tfen daha sonra tekrar deneyin."
                
        except Exception as e:
            print(f"âŒ LLM iÅŸleme hatasÄ±: {e}")
            return "Bir hata oluÅŸtu. LÃ¼tfen tekrar deneyin."
    
    def _create_full_prompt(self, user_message: str, conversation_context: str = "") -> str:
        """Tam prompt oluÅŸtur"""
        prompt_parts = [self.system_prompt]
        
        if conversation_context:
            prompt_parts.append(f"\nÃ–nceki konuÅŸmalar:\n{conversation_context}")
        
        prompt_parts.append(f"\nKullanÄ±cÄ±: {user_message}")
        prompt_parts.append("\nASYA:")
        
        return "\n".join(prompt_parts)
    
    def generate_streaming_response(self, user_message: str, conversation_context: str = ""):
        """Streaming yanÄ±t Ã¼ret (generator)"""
        try:
            full_prompt = self._create_full_prompt(user_message, conversation_context)
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": True,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "top_k": 40
                    }
                },
                stream=True,
                timeout=120
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            import json
                            data = json.loads(line)
                            if 'response' in data:
                                yield data['response']
                        except json.JSONDecodeError:
                            continue
            
        except Exception as e:
            print(f"âŒ Streaming yanÄ±t hatasÄ±: {e}")
            yield "Bir hata oluÅŸtu."
    
    def get_available_models(self) -> List[str]:
        """KullanÄ±labilir modelleri listele"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json()
                return [model['name'] for model in models.get('models', [])]
            return []
        except Exception as e:
            print(f"âŒ Model listesi hatasÄ±: {e}")
            return []
    
    def change_model(self, new_model: str) -> bool:
        """KullanÄ±lan modeli deÄŸiÅŸtir"""
        try:
            available_models = self.get_available_models()
            if new_model in available_models:
                self.model = new_model
                print(f"âœ… Model deÄŸiÅŸtirildi: {new_model}")
                return True
            else:
                print(f"âŒ Model mevcut deÄŸil: {new_model}")
                return False
        except Exception as e:
            print(f"âŒ Model deÄŸiÅŸtirme hatasÄ±: {e}")
            return False
    
    def get_model_info(self) -> Dict:
        """Mevcut model hakkÄ±nda bilgi al"""
        try:
            response = requests.post(
                f"{self.base_url}/api/show",
                json={"name": self.model}
            )
            
            if response.status_code == 200:
                return response.json()
            return {}
            
        except Exception as e:
            print(f"âŒ Model bilgisi hatasÄ±: {e}")
            return {}
    
    def is_model_ready(self) -> bool:
        """Model kullanÄ±ma hazÄ±r mÄ± kontrol et"""
        try:
            # Basit bir test mesajÄ± gÃ¶nder
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": "Test",
                    "stream": False,
                    "options": {"num_predict": 1}
                },
                timeout=10
            )
            return response.status_code == 200
        except:
            return False 